# Quantum Enhanced Retrieval-Augmented Generation (QRAG)

This project was for subject COMP8420 Assignment 3 - Major Project at Macquarie University<br>
Completed by Lucas Hofmockel-Spanakis (45422745)<br>
<br>
This project serves as a reference implementation and comparison of a novel approach to semantic text retreval.<br>
The core premise is that traditional RAG implementations use linear relationships to represent learned information, however language itself is mathematically non-linear, with the existance and usage of polysemy, compositional semantics, contextual dependencies and many more complicated inter-vocabulary relationships. These linear models use real vector spaces, linear dot product similarities and affine projections.<br>
That's where the theoretical advantage of applying a quantum complex Hilbert Space holds value. By applying the foundations of superposition, entanglement, complex amplitudes and non-commutative operations, we can create word representations that can exist in multiple semantic states, have non-local correlations between distant concepts, and create a data repsresentation that allows word order and context to exist in different states.<br>
These concepts provide our theoretical advantage for representing the non-linear aspects of natural language in a text retrieval system.<br>
<br>
The enterence to this project is through `notebook.ipynb` where details on the model architecture and implementation can be found.<br>
This project is highly dependency sensitive. As a result, it is recommend to run this project in a venv installing the dependencies from `requirements.txt`.<br>
The folder structure of the project is simple; in root exists all `.py` and the `notebook.ipynb` files. And `pdfs_academic` and `pdfs_train` exist in root as well. These folders contain seperate domain datasets of pdfs used for training and evaluating the models generated by the notebook. Uploaded here is only a small sample of the ~1000 total pdfs in `pdfs_train` and a sample of the 25 in `pdfs_academic`.<br>
<br>
The notebook was designed to run top to bottom, and contains mechanisms for caching the larger tasks.<br>
<br>
Of the cached data, this repo contains the training dataset pickle file, as well as all the trained models used in the notebooks uploaded configuration. Eg: "QX_EX.....pkl"
<br>
<br>
Dataset Credits:<br>
The PDFs used in the academic test:<br>
- https://paperswelove.org/ (https://github.com/papers-we-love/papers-we-love)
	- a-sparse-johnson-lindenstrauss-transform.pdf
	- General-self-similarityâ€”an-overview.pdf
	- toward-a-unified-theory-of-sparse-dimensionality-reduction-in-euclidean-	space.pdf
	- Understanding-Deep-Convolutional-Networks.pdf
	- advance_in_quantum_machine_learning.pdf
	- grovers_algorithm.pdf
	- shors_algorithm.pdf
- https://www.scienceopen.com
	- 1812.04948.pdf
- https://core.ac.uk
    - 55806094.pdf
    - 11400830.pdf
    - 11743455.pdf
    - 29198585.pdf
    - 84042296.pdf
    - 38026102.pdf
    - 54522528.pdf
    - 54526939.pdf
    - 16666874.pdf
    - 107878.pdf
    - 84121956.pdf
    - 84121933.pdf
    - 84121926.pdf
    - 84121954.pdf
    - qt4h46v9d9.pdf
    - 36057438.pdf
    - 287427805.pdf
- https://www.kaggle.com/datasets/manisha717/dataset-of-pdf-files
	- The over 1000 pdfs used to train the model